{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"DSAN 5000 HW4.3: Visualizing Word Vectors with t-SNE\"\n",
    "format:\n",
    "  html:\n",
    "    embed-resources: true\n",
    "    toc: true\n",
    "    df-print: kable\n",
    "    link-external-newwindow: true\n",
    "    link-external-icon: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part we've taken the 4874 words which remained in the vocabulary in our solutions to HW4.1[^1], and downloaded the [Vertex AI embedding](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings) for each.\n",
    "\n",
    "Note how the **topic-based** word vectors from HW4.2 were **4-dimensional** representations of each word in the vocabulary, that we computed simply to classify our documents into one of four topics.\n",
    "\n",
    "Vertex AI (and any other embedding API) provides much richer, higher-dimensional (in this case, **768-dimensional**) vectors, computed to encode general **semantic similarity** properties of words such that words whose vectors are closer in this 768-dimensional space are more semantically-related: the vectors encoding \"dog\" and \"cat\", for example, will be closer than those encoding \"aardvark\" and \"spectroscopy\".\n",
    "\n",
    "Your task in this part will be to **visualize** these much richer word vectors, and in the next part you will **evaluate** how well the **cosine similarity** between word vectors in this space correlates with human judgements of semantic similarity.\n",
    "\n",
    "As a final (but important!) detail: though the vectors stored by Vertex AI are indeed 768-dimensional, they also support a protocol called [Matryoshka Representation Learning](https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-announces-new-text-embedding-models), which allows users to choose any dimensionality $d < 768$ and obtain efficiently-**compressed** vectors in $d$-dimensional space (now that we've covered Dimensionality Reduction algorithms, you may have some idea of how this protocol might work!). So, to make file sizes more manageable, the vectors we provide here are **256-dimensional** Matryoshka-reduced versions of the full 768-dimensional Vertex embeddings for each word.[^2]\n",
    "\n",
    "[^1]: If your text-cleaning code in Step 1 worked slightly differently from ours, you may have gotten a different vocabulary size, with slightly more or slightly less than 4874 words, which is ok! In both this part and in HW4.4, we provide the vectors for these 4874 words, and you won't need to e.g. merge them with your vocabulary from Part 1 at all.\n",
    "\n",
    "[^2]: The results here don't change if you use the 768-dimensional versions, but if you're interested in using them elsewhere (for example, to avoid spending money if you hope to look at word embeddings as part of your final project), the non-Matryoshka-compressed 768-dimensional vectors can be downloaded [here](https://drive.google.com/drive/folders/1ykKTI7mHb0IuRK-6hc84AuiH-_kfCR6E?usp=drive_link). Though they're only 33.8 MB in total, you still should **not** include these in your submission, since they may cause your repo to hit GitHub's file size limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports and Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"hw4.ini\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# The key scikit-learn class we'll use in this part!\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# For plotting words within the 2-dimensional t-SNE space\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with the NY Times articles in HW4.1, we have pre-loaded the embeddings for the 4874 words in the vocabulary into a compressed `.csv.zip` file, which you can load into Pandas in the same way you loaded those articles in HW4.1.\n",
    "\n",
    "The URL for the file is given in the `hw4.ini` file, meaning that you should use the `config` object created in Step 1 above to obtain this URL, then use it in Pandas' `read_csv()` function to load the embeddings into a `DataFrame` object named `emb_df`.\n",
    "\n",
    "At the end of your code cell, use `emb_df.shape` to display the shape of the loaded `DataFrame`, to verify that it has 4874 rows (one per word in the vocabulary) and 256 columns (the dimensionality of the embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4874, 256)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df = pd.read_csv(\"emb_df.csv.zip\")\n",
    "emb_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fit the t-SNE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining steps are identical to the steps from HW4.2, except that this time you will project the **256-dimensional** word vectors down into a **2-dimensional** space (a much greater degree of compression than the 4D-to-2D transformation you carried out via `TSNE` in HW4.2!).\n",
    "\n",
    "So, in this step (as in HW4.2 Step 3), create a `scikit-learn` `TSNE` object named `tsne_emb_model`, then use `fit_transform()` to fit the t-SNE model to the data in `emb_df`, saving the result of the call into a new object named `tsne_emb_projections`. Use `tsne_emb_projections.shape` as the last line of the code cell to verify that you now have a $4874 \\times 2$ matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: hw4-3-3-response\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Filter t-SNE Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again mirroring HW4.2, in the following code cell please:\n",
    "\n",
    "* Construct a `DataFrame` object named `tsne_emb_df` with the same contents as `tsne_emb_projections` but with column headers `\"x\"` and `\"y\"`, then\n",
    "* Using the word weights file (whose filepath is specified in `hw4.ini`), append two additional columns `\"word\"` and `\"weight\"` to `tsne_emb_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: hw4-3-4-load-weights\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, in the following code cell, **filter** `tsne_emb_df` so that it contains the t-SNE projections for only the top $N$ words by tf-idf importance (where this $N$ is defined by the `num_words_tsne` variable in `hw4.ini`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: hw4-3-4-filter\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Plot t-SNE Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as was the case in HW4.2, running the following code cell should produce an interactive plot similar to the plot created in the previous part of the assignment.\n",
    "\n",
    "In a Markdown cell below this plot, please write one or two sentences describing the **differences** you observe between the 4D-to-2D plot from the previous part and the 256D-to-2D plot in this part. Though the points may no longer seem clustered, do close-together points in the t-SNE plot here at least still seem semantically similar?\n",
    "\n",
    "If you are unable to make sense of the plot (or if you're just curious!) you can go back up to Step 3 and specify the **perplexity** parameter in the call to the `TSNE()` constructor, then re-run Step 4 and re-generate the plot below.\n",
    "\n",
    "The [default value for `perplexity`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) is `30.0`. So, setting this parameter to values below or above `30.0` should produce plots which capture the more \"localized\" or more \"global\" clustering of points (respectively) in the original 256-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tsne_emb_df' in globals():\n",
    "    px.scatter(tsne_emb_df, x=\"x\", y=\"y\", hover_name=\"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Your response here)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
