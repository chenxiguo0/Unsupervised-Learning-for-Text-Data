{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Evaluating Embeddings with WordNet\"\n",
    "format:\n",
    "  html:\n",
    "    embed-resources: true\n",
    "    toc: true\n",
    "    df-print: kable\n",
    "    link-external-newwindow: true\n",
    "    link-external-icon: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how you used the original NY Times sections to evaluate the unsupervised topic estimation at the end of Part 1, here you will use a linguistic resource called WordNet to evaluate the (unsupervised) word embeddings you visualized in Part 3.\n",
    "\n",
    "While in part-1 and part-2 we looked at how unsupervised learning algorithms can discover meaningful latent properties of **documents** (the section of the NY Times that the article was published in), here we'll see how t-SNE can allow us to discover meaningful latent properties of **words**.\n",
    "\n",
    "Specifically, in this part we will draw on the notion that words in a given language can be clustered into [**\"Synsets\"**](https://wordnet.princeton.edu/): sets of words which have approximately the same meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports and Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global setting: Compute pairwise distances between [50] words\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"hw4.ini\")\n",
    "embeddings_url = config['ExternalFiles']['embeddings']\n",
    "num_words_wordnet = int(config.get('Globals', 'num_words_wordnet'))\n",
    "print(f\"Global setting: Compute pairwise distances between [{num_words_wordnet}] words\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# For constructing a DataFrame containing all *pairs* of words from a list of\n",
    "# individual words\n",
    "from sklearn.utils.extmath import cartesian\n",
    "\n",
    "# For computing the cosine similarity score between a pair of word vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# For computing Synset path distances\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the embedding vectors into a Pandas `DataFrame` named `emb_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4874, 256)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df = pd.read_csv(embeddings_url)\n",
    "emb_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Vectors for Top $N$ Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the `num_words_wordnet` global variable from `hw4.ini` comes in: in the following code cell, reduce the full length-4874 `emb_df` `DataFrame` down into a `DataFrame` object named `top_word_df`, by keeping only the vectors for the **top $N$** most important words. \n",
    "\n",
    "Once you have constructed `top_word_df`, use `top_word_df.shape` as the last line in your code cell, to display and verify that `top_word_df` contains $N$ rows and 256 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 256)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weights_path = config['DataPaths']['word_weights']\n",
    "weight_df = pd.read_csv(word_weights_path)\n",
    "\n",
    "top_words = weight_df.nlargest(num_words_wordnet, 'weight')['word']\n",
    "top_word_df = emb_df[emb_df.index.isin(top_words.index)]\n",
    "top_word_df = top_word_df.dropna()\n",
    "\n",
    "top_word_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Construct Word Pairs `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the reason why we need to filter down to a small number of words (`num_words_wordnet`) will become clear! Use the [`cartesian()` function](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/extmath.py#L793) from `scikit-learn`, imported in Step 1 above, to construct a `DataFrame` object named `word_pair_df`, where\n",
    "\n",
    "* Each row should represent a **pair** of words (from the `\"word\"` column of `top_word_df`),\n",
    "* The first column should be named `\"w1\"`, and\n",
    "* The second column, representing the second word in the pair, should be named `\"w2\"`.\n",
    "\n",
    "\n",
    "We will use this `DataFrame` throughout the following steps, to store the **Cosine similarities** and then the **WordNet path similarities** for each pair.\n",
    "\n",
    "In the last line of the code cell, please use `word_pair_df.shape` to verify that you now have an $N^2 \\times 2$ `DataFrame`, where $N$ represents the value of `num_words_wordnet` from `hw4.ini`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = weight_df.nlargest(num_words_wordnet, 'weight')['word'].tolist()\n",
    "word_pairs = cartesian([top_words, top_words])\n",
    "word_pair_df = pd.DataFrame(word_pairs, columns=['w1', 'w2'])\n",
    "word_pair_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compute Pairwise Cosine Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, use `scikit-learn`'s `cosine_similarity()` function (imported in Step 1 above) to compute a cosine similarity score for **all pairs** of the $N$ vectors you extracted in Step 3.\n",
    "\n",
    "The nice thing about this `cosine_similarity()` function is that, if you give it just a **single** NumPy matrix (remember that you can convert a Pandas `DataFrame` into a NumPy matrix via `df.values`), it will assume that the **rows** of this matrix represent the items which you'd like pairwise similarity score for, and compute the scores accordingly.\n",
    "\n",
    "This means that, for example, if you provide an $N \\times 256$ matrix, the function returns a new $N \\times N$ where the entry in row $i$ column $j$ represents the similarity between row $i$ and row $j$ of the originally-provided matrix.\n",
    "\n",
    "Use this fact to construct such an $N \\times N$ matrix via `cosine_similarity()`, saving the result as a variable named `pairwise_sims`.\n",
    "\n",
    "You should then be able to use the `flatten()` function from NumPy to convert this $N \\times N$ matrix into a single length-$N^2$ vector, which you should append as a new column named `\"cosine_sim\"` within the `word_pair_df` `DataFrame` object created in the previous step. (Here, like in the earlier cases where you used `pd.concat()`, the flattened version of `pairwise_sims` should have the same ordering as the word pairs in `word_pair_df`, as long as you did not re-arrange these objects at any point!)\n",
    "\n",
    "As the final line in the code cell, use `word_pair_df.head()` to display the first 5 rows, to verify that the values in the new `\"cosine_sim\"` column are valid and reasonable (for example, the similarity between a word and itself should be `1.0`!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.560577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.559096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.544717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w1  w2  cosine_sim\n",
       "0   0   0    1.000000\n",
       "1   0   1    0.868294\n",
       "2   0   2    0.560577\n",
       "3   0   3    0.559096\n",
       "4   0   4    0.544717"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_vectors = top_word_df.dropna().values\n",
    "pairwise_sims = cosine_similarity(top_vectors)\n",
    "flattened_sims = pairwise_sims.flatten()\n",
    "\n",
    "top_words_list = top_word_df.index.tolist()\n",
    "word_pairs = [(w1, w2) for i, w1 in enumerate(top_words_list) for j, w2 in enumerate(top_words_list)]\n",
    "word_pair_df = pd.DataFrame(word_pairs, columns=['w1', 'w2'])\n",
    "word_pair_df['cosine_sim'] = flattened_sims\n",
    "\n",
    "word_pair_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compute Pairwise WordNet Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we'll use `nltk`'s [programmatic interface](https://www.nltk.org/howto/wordnet.html) for [WordNet](https://wordnet.princeton.edu/), via the `wn` alias imported in Step 1 above, to obtain **human judgements** of the semantic similarities for each pair of words in `word_pair_df`.\n",
    "\n",
    "As a helper, we've provided a `get_wordnet_sim()` function for you at the beginning of the following code cell, which takes in strings `w1` and `w2`, uses the `.synsets()` function to obtain the Synset for the **most commonly-used form** of each word, then uses `wn.path_similarity()` to compute a similarity score between the two Synsets.\n",
    "\n",
    "**Note that this function returns the value `pd.NA`** when it is given word pairs where one or both words are not found in WordNet. **This will indeed be the case** for a few of the words (WordNet does *not* guarantee coverage of every word in the English language, since it is just a collaboration among linguists to cover as many words as possible given their resources!), so you should **take this into account** and **drop the rows containing `pd.NA` values** when computing the correlation coefficient in Step 7 below!\n",
    "\n",
    "Though you can find more details in the NLTK guide linked above, the `path_similarity()` function boils down to: two Synsets are similar if only a small number of \"hops\" through the human-constructed WordNet hierarchy are required to go from one to the other. Thus, for example, \"dog\" and \"cat\" will receive a high similarity score due to the small number of hops required (here, \"up\" and \"down\" refer to the **hypernyms** and **hyponyms** of each term, which you can see by opening the linked pages then clicking \"More\" and expanding the \"Hypernyms\" or \"Hyponyms\" links which appear):\n",
    "\n",
    "* From [dog](https://en-word.net/lemma/dog) up to [domestic animal](https://en-word.net/lemma/domestic%20animal), then\n",
    "* From [domestic animal](https://en-word.net/lemma/domestic%20animal) down to [house cat](https://en-word.net/lemma/house%20cat), and finally\n",
    "* From [house cat](https://en-word.net/lemma/house%20cat) up to [cat](https://en-word.net/lemma/cat)\n",
    "\n",
    "Given this approach, and the `get_wordnet_sim()` function which computes its numeric value, construct a new column in `word_pair_df` (immediately after the `cosine_sim` column) named `\"wn_sim\"`, containing the **WordNet path similarity** for each pair of words.\n",
    "\n",
    "Once this column has been added, use `word_pair_df.head()` as the last line in the code cell, to verify that the path similarity scores are reasonable, as you did for the Cosine similarity score in the previous step (for example, like with the Cosine similarity scores, the WordNet path similarity between a word and itself should be `1.0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>cosine_sim</th>\n",
       "      <th>wn_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868294</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.560577</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.559096</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.544717</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w1  w2  cosine_sim    wn_sim\n",
       "0   0   0    1.000000  1.000000\n",
       "1   0   1    0.868294  0.333333\n",
       "2   0   2    0.560577  0.333333\n",
       "3   0   3    0.559096  0.333333\n",
       "4   0   4    0.544717  0.333333"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def get_wordnet_sim(w1, w2):\n",
    "    w1_synsets = wn.synsets(w1)\n",
    "    if len(w1_synsets) == 0:\n",
    "        return pd.NA\n",
    "    w1_synset = w1_synsets[0]\n",
    "    w2_synsets = wn.synsets(w2)\n",
    "    if len(w2_synsets) == 0:\n",
    "        return pd.NA\n",
    "    w2_synset = w2_synsets[0]\n",
    "    try:\n",
    "        wn_sim = wn.path_similarity(w1_synset, w2_synset)\n",
    "    except:\n",
    "        return pd.NA\n",
    "    return wn_sim\n",
    "\n",
    "# Your code here\n",
    "word_pair_df['wn_sim'] = word_pair_df.apply(lambda row: get_wordnet_sim(str(row['w1']), str(row['w2'])), axis=1)\n",
    "word_pair_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Cosine Distance-Path Distance Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've reached the final step! In the following code cell, first **handle any `pd.NA` values that were returned by `get_wordnet_sim()`**, as described in the Step 6 instructions.\n",
    "\n",
    "Once you have the subset of `word_pair_df` containing all pairs without a `pd.NA` value for `wn_sim`, use the `.corr()` function from Pandas to compute the **Pearson correlation coefficient** between the two similarities.\n",
    "\n",
    "You should find a fairly large value, above 70%, thus illustrating that **unsupervised algorithms like the Word Embedding algorithm used by Vertex AI can \"automatically\" construct semantic vector spaces which align to a great extent with human judgements of semantic similarity, which were painstakingly constructed over many years by the professional linguists behind WordNet!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47951834397172394"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pair_df_clean = word_pair_df.dropna(subset=['wn_sim'])\n",
    "correlation = word_pair_df_clean['cosine_sim'].corr(word_pair_df_clean['wn_sim'])\n",
    "correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5400",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
